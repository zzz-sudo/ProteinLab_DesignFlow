#!/usr/bin/env python3
"""
å•åºåˆ—AlphaFold2é¢„æµ‹æµ‹è¯•å·¥å…·
åŸºäºå®˜æ–¹ColabFold notebookçš„ç®€åŒ–ç‰ˆæœ¬
ç”¨æˆ·è¾“å…¥å•æ¡åºåˆ—ï¼Œå®Œæ•´ä¸‹è½½å’Œå¤„ç†æ‰€æœ‰å¿…è¦æ–‡ä»¶
"""

import os
import re
import hashlib
import random
from datetime import datetime

# æ·»åŠ è¾…åŠ©å‡½æ•°åˆ°ç”Ÿæˆä»£ç ä¸­

def generate_single_sequence_test_code(config_data):
    """ç”Ÿæˆå•åºåˆ—æµ‹è¯•ä»£ç """
    
    code = f'''# ============================================================================
# ColabFoldå•åºåˆ—æµ‹è¯•å·¥å…· v1.0
# åŸºäºå®˜æ–¹notebookçš„ç®€åŒ–ç‰ˆæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
# ============================================================================

print("="*80)
print("ColabFoldå•åºåˆ—AlphaFold2é¢„æµ‹æµ‹è¯•")
print("="*80)

# ============================================================================
# è¾…åŠ©å‡½æ•°å®šä¹‰
# ============================================================================

def add_hash(jobname, sequence):
    """æ·»åŠ åºåˆ—å“ˆå¸Œåˆ°jobname"""
    import hashlib
    return jobname + "_" + hashlib.sha1(sequence.encode()).hexdigest()[:5]

def sanitize_jobname(name):
    """æ¸…ç†jobåç§°"""
    import re
    name = "".join(name.split())
    name = re.sub(r'[^a-zA-Z0-9_-]', '', name)
    return name

# ============================================================================
# 1. å¯¼å…¥å¿…è¦çš„åº“å’Œä¾èµ–å®‰è£…
# ============================================================================

# å®‰è£…ColabFoldï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
print("\\n=== æ­¥éª¤1: å®‰è£…ColabFold ===")
import os
import subprocess

try:
    import colabfold
    print("âœ… ColabFoldå·²å®‰è£…")
except ImportError:
    print("ğŸ“¦ å®‰è£…ColabFold...")
    subprocess.run(["pip", "install", "colabfold[alphafold-minus-jax]", "-q"], check=True)
    print("âœ… å®‰è£…å®Œæˆ")

# å¯¼å…¥å…³é”®åº“
try:
    from colabfold.batch import get_queries, run, set_model_type
    from colabfold.download import download_alphafold_params
    import concurrent.futures
    
    # TensorFlow/JAXè®¾ç½®
    import os
    os.environ["TF_FORCE_UNIFIED_MEMORY"] = "1"
    os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
    
    print("âœ… ä¾èµ–åº“å¯¼å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ å¯¼å…¥å¤±è´¥: {{e}}")
    raise

# ============================================================================
# 2. ç”¨æˆ·è¾“å…¥å’Œåºåˆ—å¤„ç†
# ============================================================================

print("\\n=== æ­¥éª¤2: åºåˆ—è¾“å…¥å’Œå¤„ç† ===")

# ç”¨æˆ·è¾“å…¥åºåˆ—ï¼ˆColabå‹å¥½ç‰ˆæœ¬ï¼‰
print("è¯·è¾“å…¥è¦é¢„æµ‹çš„è›‹ç™½è´¨åºåˆ—:")
print("ç¤ºä¾‹: PIAQIHILEGRSDEQKETLIREVSEAISRSLDAPLTSVRVIITEMAKGHFGIGGELASK")
print("\\næ”¯æŒ:")
print("- å•ä½“åºåˆ—: ç›´æ¥è¾“å…¥åºåˆ—")
print("- å¤åˆç‰©: ç”¨å†’å·åˆ†éš”é“¾ (å¦‚: SEQ1:SEQ2:SEQ3)")
print("- å¦‚æœä¸è¾“å…¥ï¼Œå°†ä½¿ç”¨é»˜è®¤æµ‹è¯•åºåˆ—")
print("\\nè¯·åœ¨ä¸‹é¢ä¿®æ”¹ query_sequence å˜é‡:")

# ç”¨æˆ·éœ€è¦åœ¨è¿™é‡Œä¿®æ”¹åºåˆ—
query_sequence = """MKHQFGCLTVKLMLWGFHVLKRLQGGNFIYQKQSPQYVQHLDLQKNKLKALVLWQDKQGQVIGTEFDDSLKKEQMQSGAHGMDLISRLKNQIQVVKEGSTDNLLQYKQDLFQVKKQLKLEKDDGLQSQDTKLKKILNAMAEKILNLLKELNQDQTQQKLIELNKEKQDLQLQDKQAQQEKQQLKYLKQLIDELNKNNKQLKELNKQILKEQKKNLQLQKKQILEQKKKQDLKEQKKNQQQLKLLNEQADKLEQLQQQEKQKDLQLEQKQKQ"""

# æ£€æŸ¥æ˜¯å¦ä¸ºç©ºæˆ–é»˜è®¤å€¼
if query_sequence.strip() == "" or "MKHQFGCLTVKLMLWGFHVLKRLQGG" in query_sequence:
    print("\\nâš ï¸  æ£€æµ‹åˆ°å¯èƒ½æœªä¿®æ”¹çš„é»˜è®¤åºåˆ—")
    print("ğŸ“ è¯·åœ¨ä¸Šæ–¹ä¿®æ”¹ query_sequence å˜é‡ä¸ºæ‚¨è¦é¢„æµ‹çš„åºåˆ—")
    print("ğŸ’¡ æˆ–è€…ç›´æ¥ä½¿ç”¨é»˜è®¤åºåˆ—è¿›è¡Œæµ‹è¯•")
    
    # ç›´æ¥ä½¿ç”¨ç®€çŸ­æµ‹è¯•åºåˆ—ï¼ˆé¿å…Colab inputé—®é¢˜ï¼‰
    print("\\nğŸ”§ è‡ªåŠ¨ä½¿ç”¨ç®€æ´æµ‹è¯•åºåˆ—")
    query_sequence = "MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLTYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIKDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITLGMDELYK"
    print("âœ… ä½¿ç”¨ç»¿è‰²è§å…‰è›‹ç™½(GFP)æµ‹è¯•åºåˆ— - 239æ®‹åŸº")

# æ¸…ç†åºåˆ—ï¼ˆç§»é™¤ç©ºæ ¼ï¼‰
query_sequence = "".join(query_sequence.split())

# åˆ›å»ºjobname
jobname = f"test_{{datetime.now().strftime('%Y%m%d_%H%M%S')}}"
jobname = add_hash(jobname, query_sequence)

print(f"\\nåºåˆ—ä¿¡æ¯:")
print(f"- é•¿åº¦: {{len(query_sequence)}} æ®‹åŸº")
print(f"- Jobåç§°: {{jobname}}")
print(f"- åºåˆ—é¢„è§ˆ: {{query_sequence[:50]}}...")

if ':' in query_sequence:
    print("\\næ£€æµ‹åˆ°å¤šé“¾å¤åˆç‰©!")
    chains = query_sequence.split(':')
    print(f"- é“¾æ•°: {{len(chains)}}")
    for i, chain in enumerate(chains, 1):
        print(f"  é“¾{{i}}: {{len(chain)}} æ®‹åŸº")

# ============================================================================
# 3. AlphaFoldå‚æ•°é…ç½®
# ============================================================================

print("\\n=== æ­¥éª¤3: AlphaFoldå‚æ•°é…ç½® ===")

# ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
PARAMS = {config_data}
num_relax = PARAMS['num_relax']
num_recycles = PARAMS['num_recycles'] 
msa_mode = PARAMS['msa_mode']
template_mode = PARAMS['template_mode']
model_type = PARAMS['model_type']
max_msa = PARAMS['max_msa']
pair_mode = PARAMS['pair_mode']
save_detail = PARAMS['save_all']
plddt_threshold = PARAMS['plddt_threshold']

print(f"é¢„æµ‹å‚æ•°:")
print(f"- ç»“æ„æ¾å¼›: {{num_relax}} ä¸ªæ¨¡å‹")
print(f"- å¾ªç¯æ¬¡æ•°: {{num_recycles}}")
print(f"- MSAæ¨¡å¼: {{msa_mode}}")
print(f"- æ¨¡æ¿æ¨¡å¼: {{template_mode}}")
print(f"- æ¨¡å‹ç±»å‹: {{model_type}}")
print(f"- æœ€å¤§MSA: {{max_msa}}")
print(f"- è´¨é‡é˜ˆå€¼: {{plddt_threshold}}")

# Amberæ¾å¼›å¼€å…³
use_amber = num_relax > 0

# ============================================================================
# 4. åˆ›å»ºä½œä¸šç›®å½•
# ============================================================================

print("\\n=== æ­¥éª¤4: å‡†å¤‡ä½œä¸šç›®å½• ===")

# åˆ›å»ºè¾“å‡ºç›®å½•
output_dir = f"./{{jobname}}"
os.makedirs(output_dir, exist_ok=True)

print(f"è¾“å‡ºç›®å½•: {{output_dir}}")

# åˆ›å»ºCSVåºåˆ—æ–‡ä»¶
csv_path = os.path.join(output_dir, f"{{jobname}}.csv")
with open(csv_path, "w") as f:
    f.write(f"id,sequence\\n{{jobname}},{{query_sequence}}")

print(f"åºåˆ—æ–‡ä»¶: {{csv_path}}")

# ============================================================================
# 5. ä¸‹è½½AlphaFoldå‚æ•°
# ============================================================================

print("\\n=== æ­¥éª¤5: ä¸‹è½½AlphaFoldå‚æ•° ===")
print("è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´...")

# å…ˆæ£€æµ‹ç”¨æˆ·å‚æ•°è®¾ç½®ï¼Œå†ä¸‹è½½
model_type_settings = PARAMS.get('model_type', 'auto')
if model_type_settings == 'auto':
    # æ ¹æ®åºåˆ—é•¿åº¦è‡ªåŠ¨åˆ¤æ–­æ¨¡å‹ç±»å‹
    param_model_type = 'alphafold2_ptm' if len(query_sequence) < 1000 else 'alphafold2_multimer_v3'
    print(f"è‡ªåŠ¨é€‰æ‹©æ¨¡å‹ç±»å‹: {{param_model_type}} (åºåˆ—é•¿åº¦: {{len(query_sequence)}})")
else:
    param_model_type = model_type_settings
    print(f"ä½¿ç”¨æŒ‡å®šæ¨¡å‹ç±»å‹: {{param_model_type}}")

# ä¸‹è½½å‚æ•°åˆ°å½“å‰ç›®å½•ï¼ˆå‚ç…§å®˜æ–¹å®ç°ï¼‰
data_dir = Path(".")
print(f"ä¸‹è½½è·¯å¾„: {{data_dir}}")

try:
    download_alphafold_params(param_model_type, data_dir)
    print("âœ… AlphaFoldå‚æ•°ä¸‹è½½å®Œæˆ")
    
    # æ£€æŸ¥å‚æ•°æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    param_files = list(data_dir.glob("params/*.npz"))
    if param_files:
        print(f"âœ… æ‰¾åˆ°å‚æ•°æ–‡ä»¶: {{len(param_files)}} ä¸ª")
        print(f"ç¤ºä¾‹æ–‡ä»¶å: {{param_files[0].name}}")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°å‚æ•°æ–‡ä»¶ï¼Œä½†ç»§ç»­å°è¯•é¢„æµ‹...")
        
except Exception as e:
    print(f"âŒ å‚æ•°ä¸‹è½½å¤±è´¥: {{e}}")
    print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥åé‡è¯•")
    print("å¸¸è§è§£å†³æ–¹æ¡ˆ:")
    print("1. æ£€æŸ¥Colabè¿è¡Œæ—¶çš„ç½‘ç»œè¿æ¥")
    print("2. Runtime -> Restart runtime åé‡è¯•")
    print("3. å°è¯•ä½¿ç”¨ä¸åŒçš„AlphaFoldæ¨¡å‹")
    raise

# ============================================================================
# 6. è·å–æŸ¥è¯¢å’Œè®¾ç½®æ¨¡å‹
# ============================================================================

print("\\n=== æ­¥éª¤6: è®¾ç½®é¢„æµ‹æ¨¡å‹ ===")

# è·å–æŸ¥è¯¢æ•°æ®
queries, is_complex = get_queries(csv_path)

print(f"æŸ¥è¯¢ä¿¡æ¯:")
print(f"- æŸ¥è¯¢æ•°é‡: {{len(queries)}}")
print(f"- å¤åˆç‰©æ¨¡å¼: {{is_complex}}")

# è®¾ç½®æ¨¡å‹ç±»å‹
actual_model_type = set_model_type(is_complex, model_type)
print(f"- å®é™…ä½¿ç”¨æ¨¡å‹: {{actual_model_type}}")

# ============================================================================
# 7. æ‰§è¡ŒAlphaFold2é¢„æµ‹
# ============================================================================

print("\\n=== æ­¥éª¤7: æ‰§è¡ŒAlphaFold2é¢„æµ‹ ===")
print(f"é¢„æµ‹å‚æ•°: {{len(query_sequence)}} æ®‹åŸºåºåˆ—")

start_time = datetime.now()
print(f"å¼€å§‹æ—¶é—´: {{start_time.strftime('%H:%M:%S')}}")

# è®¾ç½®cluster_profileé€»è¾‘ï¼ˆå‚ç…§å®˜æ–¹ä»£ç ï¼‰
if "multimer" in actual_model_type and max_msa is not None and max_msa != 'auto':
    use_cluster_profile = False
else:
    use_cluster_profile = True

# å¤„ç†max_msaå‚æ•°ï¼Œå°†'auto'è½¬æ¢ä¸ºNone
processed_max_msa = None
if max_msa and max_msa != 'auto':
    processed_max_msa = max_msa
else:
    processed_max_msa = None

# å¤„ç†num_recycleså‚æ•°ï¼Œå°†'auto'è½¬æ¢ä¸ºæ•°å€¼
if num_recycles == 'auto' or num_recycles is None:
    if "multimer" in actual_model_type:
        processed_num_recycles = 20  # å¤åˆç‰©é»˜è®¤20æ¬¡
    else:
        processed_num_recycles = 3   # å•ä½“é»˜è®¤3æ¬¡
else:
    processed_num_recycles = int(num_recycles)

try:
    # æ‰§è¡Œé¢„æµ‹ï¼ˆå®Œå…¨æŒ‰ç…§å®˜æ–¹notebookçš„å‚æ•°ï¼‰
    results = run(
        queries=queries,
        result_dir=output_dir,
        use_templates=False,
        custom_template_path=None,
        num_relax=num_relax if use_amber else 0,
        msa_mode=msa_mode,
        model_type=actual_model_type,
        num_models=5,
        num_recycles=processed_num_recycles,
        relax_max_iterations=200,
        recycle_early_stop_tolerance=0.0,
        num_seeds=1,
        use_dropout=False,
        model_order=[1,2,3,4,5],
        is_complex=is_complex,
        data_dir=Path("."),
        keep_existing_results=False,
        rank_by="auto",
        pair_mode=pair_mode if is_complex else "unpaired",
        pairing_strategy="greedy",
        stop_at_score=float(100),
        dpi=200,
        zip_results=False,
        save_all=save_detail in ['detailed', 'comprehension'],
        max_msa=processed_max_msa,
        use_cluster_profile=use_cluster_profile,
        input_features_callback=lambda x: None,
        prediction_callback=lambda *args: None,
        use_folding_cache=False
    )
    
    end_time = datetime.now()
    duration = end_time - start_time
    print(f"\\nâœ… é¢„æµ‹å®Œæˆ!")
    print(f"è€—æ—¶: {{duration.seconds}} ç§’")

except Exception as e:
    print(f"\\nâŒ é¢„æµ‹å¤±è´¥: {{e}}")
    if 'jax.core' in str(e).lower() or 'jax' in str(e).lower():
        print("\\nğŸ”„ æ£€æµ‹åˆ°JAXé”™è¯¯ï¼Œå»ºè®®:")
        print("1. Runtime -> Restart runtime")
        print("2. é‡æ–°è¿è¡Œæ­¤notebook")
        raise RuntimeError("JAXç¯å¢ƒé”™è¯¯ï¼Œéœ€è¦é‡å¯runtime")
    else:
        print(f"\\nğŸ“‹ å®Œæ•´é”™è¯¯ä¿¡æ¯:")
        print(f"{{e}}")
        raise

# ============================================================================
# 8. åˆ†æé¢„æµ‹ç»“æœ
# ============================================================================

print("\\n=== æ­¥éª¤8: åˆ†æé¢„æµ‹ç»“æœ ===")

# æŸ¥æ‰¾ç»“æœæ–‡ä»¶
import glob

# æŸ¥æ‰¾JSONç»“æœæ–‡ä»¶
json_files = glob.glob(os.path.join(output_dir, "*.json"))
pdb_files = glob.glob(os.path.join(output_dir, "*.pdb"))

print(f"ç”Ÿæˆçš„æ–‡ä»¶:")
print(f"- JSONç»“æœ: {{len(json_files)}} ä¸ª")
print(f"- PDBç»“æ„: {{len(pdb_files)}} ä¸ª")

if json_files:
    # è¯»å–scores.json
    scores_file = os.path.join(output_dir, f"{{jobname}}_scores.json")
    if os.path.exists(scores_file):
        import json
        with open(scores_file, 'r') as f:
            scores_data = json.load(f)
        
        print(f"\\né¢„æµ‹åˆ†æ•°:")
        for i, model in enumerate(scores_data.get('plddt', []), 1):
            avg_plddt = sum(model) / len(model)
            print(f"- æ¨¡å‹{{i}}: pLDDT = {{avg_plddt:.2f}}")
        
        # æ‰¾åˆ°æœ€ä½³æ¨¡å‹
        best_model_idx = 0
        best_score = max([sum(score) / len(score) for score in scores_data.get('plddt', [])])
        print(f"\\nğŸ† æœ€ä½³æ¨¡å‹: æ¨¡å‹{{best_model_idx + 1}} (pLDDT: {{best_score:.2f}})")

# ============================================================================
# 9. å¯è§†åŒ–ç»“æœ
# ============================================================================

print("\\n=== æ­¥éª¤9: ç”Ÿæˆå¯è§†åŒ– ===")

try:
    import matplotlib.pyplot as plt
    import numpy as np
    
    # å¦‚æœæœ‰scoresæ•°æ®ï¼Œç»˜åˆ¶pLDDTå›¾
    if 'scores_data' in locals() and 'plddt' in scores_data:
        fig, axes = plt.subplots(1, min(5, len(scores_data['plddt'])), figsize=(12, 4))
        if len(scores_data['plddt']) == 1:
            axes = [axes]
        
        for i, plddt_scores in enumerate(scores_data['plddt'][:5]):
            axes[i].plot(plddt_scores, alpha=0.8)
            axes[i].set_title(f'æ¨¡å‹{{i+1}}')
            axes[i].set_xlabel('æ®‹åŸºä½ç½®')
            axes[i].set_ylabel('pLDDTåˆ†æ•°')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'plddt_plots.png'), dpi=150, bbox_inches='tight')
        print("âœ… pLDDTå›¾è¡¨å·²ä¿å­˜")
        
except Exception as e:
    print(f"âš ï¸ å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {{e}}")

# ============================================================================
# 10. ä¿å­˜ç»“æœå’Œä¸‹è½½
# ============================================================================

print("\\n=== æ­¥éª¤10: æ‰“åŒ…å’Œä¸‹è½½ç»“æœ ===")

# åˆ›å»ºç»“æœåŒ…
import zipfile

zip_name = f"{{jobname}}_results.zip"
with zipfile.ZipFile(zip_name, 'w') as zipf:
    # æ·»åŠ æ‰€æœ‰ç»“æœæ–‡ä»¶
    for file_path in glob.glob(os.path.join(output_dir, "*")):
        if os.path.isfile(file_path):
            zipf.write(file_path, os.path.basename(file_path))

print(f"âœ… ç»“æœåŒ…å·²åˆ›å»º: {{zip_name}}")

# å°è¯•ä¸‹è½½
try:
    from google.colab import files
    files.download(zip_name)
    print(f"âœ… æ–‡ä»¶å·²ä¸‹è½½: {{zip_name}}")
except ImportError:
    print("âš ï¸ ä¸åœ¨Colabç¯å¢ƒä¸­ï¼Œè¯·æ‰‹åŠ¨ä¸‹è½½æ–‡ä»¶")

# ============================================================================
# å®Œæˆæ€»ç»“
# ============================================================================

print("\\n" + "="*80)
print("ğŸ‰ å•åºåˆ—AlphaFold2é¢„æµ‹å®Œæˆ!")
print("="*80)
print(f"Jobåç§°: {{jobname}}")
print(f"åºåˆ—é•¿åº¦: {{len(query_sequence)}} æ®‹åŸº")
print(f"è¾“å‡ºç›®å½•: {{output_dir}}")
print(f"ç»“æœåŒ…: {{zip_name}}")
print("\\nğŸ“‹ ç”Ÿæˆçš„æ–‡ä»¶:")
print("- PDBç»“æ„æ–‡ä»¶ (*.pdb)")
print("- é¢„æµ‹åˆ†æ•° (*.json)")
if 'scores_data' in locals():
    best_score = max([sum(score) / len(score) for score in scores_data.get('plddt', [])])
    print(f"- æœ€ä½³é¢„æµ‹è´¨é‡: {{best_score:.2f}} pLDDT")
print("- å¯è§†åŒ–å›¾è¡¨ (*.png)")
print("- å‹ç¼©ç»“æœåŒ… (*.zip)")
print("\\nâœ… å®Œæˆåè¯·æ£€æŸ¥é¢„æµ‹è´¨é‡ï¼Œåˆç†çš„pLDDTåˆ†æ•°åº” > 70")
print("="*80)
'''

    return code

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("å•åºåˆ—AlphaFold2é¢„æµ‹ä»£ç ç”Ÿæˆå™¨")
    print("="*80)
    
    # é…ç½®å‚æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰
    config_data = {
        "num_relax": 1,
        "num_recycles": "auto",
        "msa_mode": "mmseqs2_uniref_env", 
        "template_mode": "none",
        "model_type": "auto",
        "max_msa": "auto",
        "pair_mode": "unpaired_paired",
        "save_all": "detailed",
        "plddt_threshold": 70
    }
    
    print("\nğŸ“ é…ç½®å‚æ•°:")
    print("- ç»“æ„æ¾å¼›: 1ä¸ªæ¨¡å‹")
    print("- MSAæ¨¡å¼: MMseqs2 + UniRef + ç¯å¢ƒåºåˆ—")
    print("- æ¨¡æ¿æ¨¡å¼: æ— æ¨¡æ¿(ä»å¤´é¢„æµ‹)")
    print("- æ¨¡å‹ç±»å‹: è‡ªåŠ¨é€‰æ‹©")
    print("- è´¨é‡é˜ˆå€¼: 70")
    
    print(f"\nğŸ”„ ç”Ÿæˆä»£ç ...")
    
    # ç”Ÿæˆä»£ç 
    generated_code = generate_single_sequence_test_code(config_data)
    
    # ä¿å­˜æ–‡ä»¶
    output_path = "F:\\Project\\è›‹ç™½è´¨è®¾è®¡\\designs\\iter1\\single_sequence_test.py"
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(generated_code)
    
    print(f"âœ… ä»£ç å·²ä¿å­˜åˆ°: {output_path}")
    print(f"\nğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
    print(f"1. ä¸Šä¼  {output_path} åˆ°Google Colab")
    print(f"2. è¿è¡Œnotebook")
    print(f"3. è¾“å…¥è¦é¢„æµ‹çš„è›‹ç™½è´¨åºåˆ—")
    print(f"4. ç­‰å¾…é¢„æµ‹å®Œæˆåä¸‹è½½ç»“æœ")
    
    # ä¹Ÿä¿å­˜é…ç½®
    config_path = "F:\\Project\\è›‹ç™½è´¨è®¾è®¡\\designs\\iter1\\single_sequence_config.json"
    import json
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    print(f"\nğŸ‰ å•åºåˆ—æµ‹è¯•å·¥å…·ç”Ÿæˆå®Œæˆ!")
    print("="*80)

if __name__ == "__main__":
    main()
