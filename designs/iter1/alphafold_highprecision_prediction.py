# ColabFold AlphaFold2MSA é«˜ç²¾åº¦é¢„æµ‹ - å®Œæ•´ç‰ˆ
# é¡¹ç›®: highprecision_prediction
# åºåˆ—æ¥æº: ProteinMPNN + ESM-IF
# ç”Ÿæˆæ—¶é—´: 2025-10-03 17:41:29
# ä½œè€…: ColabFoldå‚æ•°é…ç½®å·¥å…·

print("="*60)
print("ColabFold AlphaFold2MSA è›‹ç™½è´¨ç»“æ„é¢„æµ‹")
print(f"é¡¹ç›®: highprecision_prediction")
print(f"åºåˆ—æ¥æº: ProteinMPNN + ESM-IF")
print(f"é¢„æµ‹åºåˆ—æ•°: ç”¨æˆ·è‡ªå®šä¹‰")
print(f"MSAæ¨¡å¼: mmseqs2_uniref_env")
print(f"æ¨¡æ¿æ¨¡å¼: none")
print("="*60)

# ================================
# 1. ç¯å¢ƒè®¾ç½®å’Œä¾èµ–å®‰è£…
# ================================

import os
import sys
from sys import version_info
python_version = f"{version_info.major}.{version_info.minor}"

USE_AMBER = True
USE_TEMPLATES = False
PYTHON_VERSION = python_version

if not os.path.isfile("COLABFOLD_READY"):
  print("installing colabfold...")
  os.system("pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'")
  if os.environ.get('TPU_NAME', False) != False:
    os.system("pip uninstall -y jax jaxlib")
    os.system("pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html")
  os.system("ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold")
  os.system("ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold")
  # hack to fix TF crash
  os.system("rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so")
  os.system("touch COLABFOLD_READY")

if USE_AMBER or USE_TEMPLATES:
  if not os.path.isfile("CONDA_READY"):
    print("installing conda...")
    os.system("wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh")
    os.system("bash Miniforge3-Linux-x86_64.sh -bfp /usr/local")
    os.system("mamba config --set auto_update_conda false")
    os.system("touch CONDA_READY")

if USE_TEMPLATES and not os.path.isfile("HH_READY") and USE_AMBER and not os.path.isfile("AMBER_READY"):
  print("installing hhsuite and amber...")
  os.system(f"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer")
  os.system("touch HH_READY")
  os.system("touch AMBER_READY")
else:
  if USE_TEMPLATES and not os.path.isfile("HH_READY"):
    print("installing hhsuite...")
    os.system(f"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'")
    os.system("touch HH_READY")
  if USE_AMBER and not os.path.isfile("AMBER_READY"):
    print("installing amber...")
    os.system(f"mamba install -y -c conda-forge openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer")
    os.system("touch AMBER_READY")

# For some reason we need that to get pdbfixer to import
if USE_AMBER and f"/usr/local/lib/python{python_version}/site-packages/" not in sys.path:
    sys.path.insert(0, f"/usr/local/lib/python{python_version}/site-packages/")

print("OK ä¾èµ–å®‰è£…å®Œæˆ")

# å®˜æ–¹TroubleshootingæŒ‡å¯¼
print("\n=== é‡è¦: å®˜æ–¹troubleshootingæŒ‡å¯¼ ===")
print("æ ¹æ®ColabFoldå®˜æ–¹æ–‡æ¡£:")
print("å¦‚æœé‡åˆ°JAXå…¼å®¹æ€§é”™è¯¯æˆ–é¢„æµ‹å¤±è´¥ï¼Œè¯·:")
print("")
print("ğŸ”„ è§£å†³æ–¹æ¡ˆ (å®˜æ–¹æ¨è):")
print("1. Runtime -> Restart runtime")
print("2. æˆ–è€…: Runtime -> Factory reset runtime") 
print("3. ç„¶å: Runtime -> Run all")
print("")
print("è¿™æ˜¯Google Colabç¯å¢ƒå·²çŸ¥çš„é—®é¢˜")
print("å®˜æ–¹æ–‡æ¡£å¯¹æ­¤æä¾›äº†æ˜ç¡®çš„troubleshootingæŒ‡å¯¼")
print("=================================\n")

# Import necessary packages
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
try:
    from Bio import BiopythonDeprecationWarning
    warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)
except:
    pass
from pathlib import Path
from colabfold.download import download_alphafold_params, default_data_dir
from colabfold.utils import setup_logging
from colabfold.batch import get_queries, run, set_model_type
from colabfold.plot import plot_msa_v2

import os
import numpy as np
import json
import zipfile
from datetime import datetime
import glob
import re

# Check for K80 GPU limitations
try:
  K80_chk = os.popen('nvidia-smi | grep "Tesla K80" | wc -l').read()
except:
  K80_chk = "0"
  pass
if "1" in K80_chk:
  print("WARNING: found GPU Tesla K80: limited to total length < 1000")
  if "TF_FORCE_UNIFIED_MEMORY" in os.environ:
    del os.environ["TF_FORCE_UNIFIED_MEMORY"]
  if "XLA_PYTHON_CLIENT_MEM_FRACTION" in os.environ:
    del os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"]

from pathlib import Path
import matplotlib.pyplot as plt
import py3Dmol
from IPython.display import display, HTML
import base64
from html import escape
from google.colab import files

# Try to import plot_protein from different locations
try:
    from colabfold.colabfold import plot_protein
    print("OK plot_protein imported from colabfold.colabfold")
except ImportError:
    try:
        from colabfold.plot import plot_protein
        print("OK plot_protein imported from colabfold.plot")
    except ImportError:
        try:
            from colabfold.utils import plot_protein
            print("OK plot_protein imported from colabfold.utils")
        except ImportError:
            print("Warning: plot_protein not available from any location")
            plot_protein = None

print("OK ç¯å¢ƒè®¾ç½®å®Œæˆ")

# ================================
# 2. åºåˆ—æ–‡ä»¶ä¸Šä¼ å’Œè§£æ
# ================================
print("\n= åºåˆ—æ–‡ä»¶ä¸Šä¼  =")

print("è¯·ä¸Šä¼ åºåˆ—æ–‡ä»¶:")
print("1. ä¸Šä¼  proteinmpnn_sequences.json")
print("2. ä¸Šä¼  esm_if_sequences.json")
print()

# ä¸Šä¼ æ–‡ä»¶
uploaded = files.upload()
uploaded_files = list(uploaded.keys())

if not uploaded_files:
    print("é”™è¯¯: æ²¡æœ‰ä¸Šä¼ ä»»ä½•æ–‡ä»¶ï¼")
    assert False, "è¯·ä¸Šä¼ åºåˆ—æ–‡ä»¶"

print(f"å·²ä¸Šä¼  {len(uploaded_files)} ä¸ªæ–‡ä»¶: {uploaded_files}")

# æ™ºèƒ½è§£æåºåˆ—æ–‡ä»¶
all_sequences = []
sequence_info = []

for filename in uploaded_files:
    print(f"\nè§£ææ–‡ä»¶: {filename}")
    
    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
    with open(filename, 'w', encoding='utf-8') as f:
        if hasattr(uploaded[filename], 'decode'):
            f.write(uploaded[filename].decode('utf-8'))
        else:
            f.write(uploaded[filename])
    
    with open(filename, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è§£æä¸åŒæ ¼å¼çš„JSONæ–‡ä»¶
    print(f"JSONæ•°æ®ç±»å‹: {type(data)}")
    if isinstance(data, dict):
        print(f"é¡¶çº§é”®æ•°é‡: {len(data)}")
        
        sequences_found = 0
        if 'results' in data:  # ESM-IFç»“æ„
            for backbone_id, backbone_data in data['results'].items():
                if 'sequences' in backbone_data:
                    sequences = backbone_data['sequences']
                    print(f"  éª¨æ¶{backbone_id}: {len(sequences)} ä¸ªåºåˆ—")
                    for seq_data in sequences:
                        if isinstance(seq_data, dict):
                            sequence = seq_data.get('sequence', '')
                            if len(sequence) > 10:
                                all_sequences.append(sequence)
                                sequence_info.append({
                                    'sequence_id': seq_data.get('sequence_id', ''),
                                    'sequence': sequence,
                                    'method': 'ESM-IF',
                                    'backbone_id': backbone_id,
                                    'length': len(sequence),
                                    'original_data': seq_data
                                })
                                sequences_found += 1
        else:  # ProteinMPNNç»“æ„
            for backbone_id, backbone_data in data.items():
                if 'sequences' in backbone_data:
                    sequences = backbone_data['sequences']
                    print(f"  éª¨æ¶{backbone_id}: {len(sequences)} ä¸ªåºåˆ—")
                    for seq_data in sequences:
                        if isinstance(seq_data, dict):
                            sequence = seq_data.get('sequence', '')
                            if len(sequence) > 10:
                                all_sequences.append(sequence)
                                sequence_info.append({
                                    'sequence_id': seq_data.get('sequence_id', ''),
                                    'sequence': sequence,
                                    'method': 'ProteinMPNN',
                                    'backbone_id': backbone_id,
                                    'length': len(sequence),
                                    'original_data': seq_data
                                })
                                sequences_found += 1
    
    print(f"ä» {filename} ä¸­æå–åˆ° {sequences_found} ä¸ªåºåˆ—")

if not all_sequences:
    print("é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è›‹ç™½è´¨åºåˆ—ï¼")
    assert False, "è¯·æ£€æŸ¥JSONæ–‡ä»¶æ ¼å¼"

print(f"\næ€»å…±æå–åˆ° {len(all_sequences)} ä¸ªåºåˆ—ç”¨äºé¢„æµ‹")

# ================================
# 3. é¢„æµ‹å‚æ•°è®¾ç½®
# ================================
print("\n= ColabFold å‚æ•°è®¾ç½® =")

# ç”¨æˆ·é…ç½®çš„å‚æ•°
jobname = "highprecision_prediction"
num_relax = 1
template_mode = "none"
msa_mode = "mmseqs2_uniref_env"
pair_mode = "unpaired_paired"
model_type = "auto"
num_recycles = None
max_msa = None
plddt_threshold = 70
save_detail = "detailed"

use_amber = num_relax > 0
use_templates = template_mode != "none"

# å‚æ•°éªŒè¯
print(f"ä»»åŠ¡åç§°: {jobname}")
print(f"æ¾å¼›æ¬¡æ•°: {num_relax}")
print(f"æ¨¡æ¿æ¨¡å¼: {template_mode}")
print(f"MSAæ¨¡å¼: {msa_mode}")
print(f"é…å¯¹æ¨¡å¼: {pair_mode}")
print(f"æ¨¡å‹ç±»å‹: {model_type}")
print(f"å¾ªç¯æ¬¡æ•°: {num_recycles}")
print(f"æœ€å¤§MSA: {max_msa}")
print(f"è´¨é‡é˜ˆå€¼: {plddt_threshold}")

# ================================
# 4. å•åºåˆ—æµ‹è¯•æ¨¡å¼
# ================================
print(f"\n= å•åºåˆ—å®Œæ•´æµç¨‹ç­–ç•¥ =")
print(f"æ€»åºåˆ—æ•°é‡: {len(all_sequences)} ä¸ª")

# æ·»åŠ å•åºåˆ—æµ‹è¯•æ¨¡å¼
TEST_SINGLE_SEQUENCE = True  # è®¾ç½®ä¸ºTrueå…ˆæµ‹è¯•å•ä¸ªåºåˆ—
print(f"ğŸ”¬ æµ‹è¯•æ¨¡å¼: {'å¼€å¯' if TEST_SINGLE_SEQUENCE else 'å…³é—­'}")

if TEST_SINGLE_SEQUENCE:
    print(f"æµ‹è¯•é…ç½®:")
    print(f"- åªæµ‹è¯•ç¬¬ä¸€æ¡åºåˆ—")
    print(f"- å®Œæ•´èµ°å®Œæ•´ä¸ªé¢„æµ‹æµç¨‹")
    print(f"- éªŒè¯æ‰€æœ‰åŠŸèƒ½æ­£å¸¸åå†å¤„ç†å…¨éƒ¨åºåˆ—")
    test_sequences = all_sequences[:1]  # åªå–ç¬¬ä¸€æ¡
else:
    print(f"å®Œæ•´å¤„ç†: {len(all_sequences)} ä¸ªåºåˆ—")
    test_sequences = all_sequences

print(f"\nğŸ’¡ ç­–ç•¥:")
print(f"- æ¯ä¸ªåºåˆ—éƒ½æ˜¯ç‹¬ç«‹çš„å®Œæ•´é¢„æµ‹")
print(f"- åŒ…æ‹¬: MSAæœç´¢ -> AlphaFoldé¢„æµ‹ -> ç»“æ„åˆ†æ -> å›¾è¡¨ç”Ÿæˆ -> ç»“æœä¿å­˜")
print(f"- é¢„ä¼°æ—¶é—´: {len(test_sequences) * 3} åˆ†é’Ÿ")
print(f"==================================================")

# åˆ›å»ºè¾“å‡ºç›®å½•
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_dir = f"{jobname}_{timestamp}"
os.makedirs(output_dir, exist_ok=True)

# å­˜å‚¨æ‰€æœ‰ç»“æœ
all_results = []
prediction_summary = {
    'successful': 0,
    'failed': 0,
    'avg_plddt_list': [],
    'method_stats': {
        'ESM-IF': 0,
        'ProteinMPNN': 0
    }
}

# å‡†å¤‡æµ‹è¯•åºåˆ—
if TEST_SINGLE_SEQUENCE:
    # åªæµ‹è¯•ç¬¬ä¸€æ¡åºåˆ—
    test_seq_data = all_sequences[0]
    sequence_info = [{
        'sequence_id': test_seq_data['sequence_id'],
        'sequence': test_seq_data['sequence'],
        'length': test_seq_data['length'],
        'method': test_seq_data['method']
    }]
    print(f"\nğŸ§ª æµ‹è¯•åºåˆ—: {test_seq_data['sequence_id']} ({test_seq_data['length']} æ®‹åŸº)")
else:
    # å¤„ç†å…¨éƒ¨åºåˆ—
    sequence_info = JSON_DATA_SEQUENCES

# å•åºåˆ—å®Œæ•´æµç¨‹é¢„æµ‹æ‰§è¡Œï¼ˆæ¨¡ä»¿å®˜æ–¹ColabFold notebookï¼‰
for i, seq_info in enumerate(sequence_info):
    print(f"\n=== åºåˆ— {i+1}/{len(sequence_info)} å®Œæ•´é¢„æµ‹æµç¨‹ ===")
    print(f"åºåˆ—ID: {seq_info['sequence_id']}")
    print(f"é•¿åº¦: {seq_info['length']} æ®‹åŸº")
    print(f"æ–¹æ³•: {seq_info['method']}")
    
    # Sanitize job name to avoid special characters
    single_jobname = re.sub(r'[^a-zA-Z0-9_-]', '_', seq_info['sequence_id'])
    sequence = seq_info['sequence']
    
    print(f"å¼€å§‹é¢„æµ‹: {single_jobname}")
    try:
        print("\n=== æ­¥éª¤1: å‡†å¤‡é¢„æµ‹ç¯å¢ƒ ===")
        
        # åˆ›å»ºå½“å‰åºåˆ—çš„è¾“å‡ºç›®å½•
        result_dir = os.path.join(output_dir, single_jobname)
        os.makedirs(result_dir, exist_ok=True)
        
        print("\n=== æ­¥éª¤2: ä¸‹è½½AlphaFoldå‚æ•° ===")
        download_alphafold_params()
        
        print("\n=== æ­¥éª¤3: å‡†å¤‡æŸ¥è¯¢æ•°æ® ===")
        # åˆ›å»ºCSVæŸ¥è¯¢æ–‡ä»¶
        queries_path = os.path.join(result_dir, f"{single_jobname}.csv")
        with open(queries_path, "w") as text_file:
            text_file.write(f"id,sequence\n{single_jobname},{sequence}")
        
        # è·å–æŸ¥è¯¢æ•°æ®
        queries, is_complex = get_queries(queries_path)
        
        print("\n=== æ­¥éª¤4: è®¾ç½®æ¨¡å‹ç±»å‹ ===")
        actual_model_type = set_model_type(is_complex, model_type)
        print(f"ä½¿ç”¨æ¨¡å‹: {actual_model_type}")
        
        print(f"\n=== æ­¥éª¤5: æ‰§è¡ŒAlphaFold2é¢„æµ‹ ===")
        print(f"åºåˆ—é•¿åº¦: {len(sequence)} æ®‹åŸº")
        
        # ç®€åŒ–çš„é¢„æµ‹è°ƒç”¨ï¼ˆæ¨¡ä»¿å®˜æ–¹notebookï¼‰
        try:
            results = run(
                queries=queries,
                result_dir=result_dir,
                num_models=5,
                num_recycles=num_recycles,
                msa_mode=msa_mode,
                num_relax=num_relax,
                save_recycles=save_detail == 'comprehensive',
                user_agent="colabfold/google-colab-main",
                calc_extra_ptm=is_complex
            )
        except Exception as pred_err:
            if 'JaxprEqn' in str(pred_err) or 'JAX' in str(pred_err):
                print(f"\n!!! JAXé”™è¯¯æ£€æµ‹ !!!")
                print(f"é”™è¯¯: {pred_err}")
                print(f"\nå»ºè®®æ–¹æ¡ˆ:")
                print(f"1. Runtime -> Restart runtime")
                print(f"2. é‡æ–°è¿è¡Œæ­¤notebook")
                print(f"3. æ­¤åºåˆ—æ ‡è®°ä¸ºå¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ª")
                metrics = None
            else:
                print(f"é¢„æµ‹é”™è¯¯: {pred_err}")
                metrics = None
        
        # å¦‚æœæ²¡æœ‰é¢„æµ‹æˆåŠŸï¼Œåˆ›å»ºå¤±è´¥ç»“æœ
        if 'results' not in locals():
            metrics = None
        else:
            print("\n=== æ­¥éª¤6: åˆ†æé¢„æµ‹ç»“æœ ===")
            metrics = extract_prediction_metrics(results, result_dir, single_jobname, seq_info)
        
        # æ¸…ç†èµ„æºï¼ˆç®€åŒ–ç‰ˆï¼‰
        print("\n=== æ­¥éª¤7: æ¸…ç†ç¯å¢ƒçŠ¶æ€ ===")
        import gc
        gc.collect()
        try:
            import jax
            jax.clear_caches()
        except:
            pass
        print("ç¯å¢ƒæ¸…ç†å®Œæˆ")
        
        # æ›´æ–°ç»Ÿè®¡
        
        if metrics and metrics['success']:
            prediction_summary['successful'] += 1
            prediction_summary['method_stats'][seq_info['method']] += 1
            if metrics.get('avg_plddt'):
                prediction_summary['avg_plddt_list'].append(metrics['avg_plddt'])
            print(f"OK åºåˆ—{i+1}é¢„æµ‹å®Œæˆ - pLDDT: {metrics.get('avg_plddt', 'N/A')}")
        else:
            prediction_summary['failed'] += 1
            print(f"å¤±è´¥ åºåˆ—{i+1}é¢„æµ‹å¤±è´¥")
        
        # ä¿å­˜ç»“æœ
        final_result = {**seq_info, **metrics} if metrics else seq_info
        all_results.append(final_result)
        
    except Exception as e:
        print(f"å¤±è´¥ åºåˆ—{i+1}é¢„æµ‹å¤±è´¥: {e}")
        failed_result = {**seq_info, 'success': False, 'error': str(e)}
        all_results.append(failed_result)
        prediction_summary['failed'] += 1
        
        # ğŸš¨ å…³é”®ï¼šæ£€æµ‹åˆ°JAXé”™è¯¯å°±æé†’ç”¨æˆ·
        if 'JAX' in str(e) or 'jax.core' in str(e):
            print(f"\nğŸš¨ JAXé”™è¯¯æ£€æµ‹åˆ°!")
            print(f"å½“å‰åºåˆ—é¢„æµ‹å¤±è´¥")
            if failed_in_batch >= 3:
                print(f"\nğŸ’¡ å»ºè®®ç«‹å³æ‰§è¡Œ:")
                print(f"1. Runtime -> Restart runtime")
                print(f"2. é‡æ–°è¿è¡Œnotebook")
                print(f"3. ä¿®æ”¹BATCH_SIZEä¸ºæ›´å°å€¼(å¦‚10)")
                print(f"\nâ¹ï¸ å½“å‰æ‰¹æ¬¡å¤±è´¥ç‡è¿‡é«˜ï¼Œå»ºè®®é‡å¯runtime")
                # è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
                try:
                    user_choice = input("æ£€æµ‹åˆ°å¤šæ¬¡JAXé”™è¯¯ï¼Œæ˜¯å¦ç»§ç»­? (y/n) [å»ºè®®n]: ").strip().lower()
                    if user_choice != 'y':
                        print("ğŸ›‘ ç”¨æˆ·é€‰æ‹©åœæ­¢ - ä¸ºé¿å…æ›´å¤šå¤±è´¥ï¼Œå»ºè®®é‡å¯runtime")
                        break
                except:
                    # åœ¨Colabç¯å¢ƒä¸­inputå¯èƒ½ä¸å¯ç”¨
                    print("âš ï¸ è‡ªåŠ¨æ£€æµ‹ï¼šJAXé”™è¯¯å¤šå‘ï¼Œå»ºè®®æ‰‹åŠ¨åœæ­¢å¹¶é‡å¯runtime")

print(f"\n=== æ€»ä½“é¢„æµ‹æ€»ç»“ ===")
print(f"æˆåŠŸ: {prediction_summary['successful']}/{len(all_results)} ä¸ªåºåˆ—")
print(f"å¤±è´¥: {prediction_summary['failed']}/{len(all_results)} ä¸ªåºåˆ—")

print(f"\nğŸ“Š å•åºåˆ—æµç¨‹è¯´æ˜:")
print(f"- æ¯ä¸ªåºåˆ—éƒ½æ˜¯ç‹¬ç«‹å®Œæ•´çš„é¢„æµ‹æµç¨‹")
print(f"- æ¨¡ä»¿å®˜æ–¹ColabFold notebookçš„åšæ³•")
print(f"- å¦‚é‡åˆ°JAXé”™è¯¯ï¼Œå»ºè®®é‡å¯runtimeåç»§ç»­")

if prediction_summary['avg_plddt_list']:
    avg_plddt = np.mean(prediction_summary['avg_plddt_list'])
    print(f"å¹³å‡pLDDT: {avg_plddt:.2f}")
    print(f"æœ€é«˜pLDDT: {np.max(prediction_summary['avg_plddt_list']):.2f}")
    print(f"æœ€ä½pLDDT: {np.min(prediction_summary['avg_plddt_list']):.2f}")

# ================================
# 5. æŒ‡æ ‡æå–å’Œåˆ†æ
# ================================
def extract_prediction_metrics(results, job_dir, sequence_id, seq_info):
    """ä»é¢„æµ‹ç»“æœä¸­æå–æŒ‡æ ‡"""
    metrics = {
        'sequence_id': sequence_id,
        'success': True,
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    }
    
    try:
        # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
        json_files = glob.glob(os.path.join(job_dir, "*.json"))
        pdb_files = glob.glob(os.path.join(job_dir, "*unrelaxed*.pdb"))
        
        # æå–scores.jsonä¸­çš„æŒ‡æ ‡
        scores_data = {}
        for json_file in json_files:
            if 'scores' in json_file.lower():
                with open(json_file, 'r') as f:
                    scores_data = json.load(f)
                break
        
        # æå–æŒ‡æ ‡
        if scores_data:
            if isinstance(scores_data, dict) and 'plddt' in scores_data:
                metrics['avg_plddt'] = float(np.mean(scores_data['plddt']))
                metrics['min_plddt'] = float(np.min(scores_data['plddt']))
                metrics['max_plddt'] = float(np.max(scores_data['plddt']))
            if isinstance(scores_data, dict) and 'ptm' in scores_data:
                metrics['ptm'] = float(scores_data['ptm'])
        
        # åˆå¹¶åŸå§‹æ•°æ®ä¸­çš„æŒ‡æ ‡
        original_data = seq_info.get('original_data', {})
        metrics.update({
            'mpnn_score': original_data.get('mpnn_score'),
            'original_plddt': original_data.get('plddt'),
            'original_ptm': original_data.get('ptm'),
            'original_pae': original_data.get('pae'),
            'original_rmsd': original_data.get('rmsd')
        })
        
        # è®¾ç½®é»˜è®¤å€¼
        metrics.setdefault('avg_plddt', None)
        metrics.setdefault('structure_file', pdb_files[0] if pdb_files else None)
        
        return metrics
        
    except Exception as e:
        print(f"æŒ‡æ ‡æå–å¤±è´¥ {sequence_id}: {e}")
        return {
            'sequence_id': sequence_id,
            'success': False,
            'error': f'æŒ‡æ ‡æå–å¤±è´¥: {e}'
        }

# ================================
# 6. ä¿å­˜ç»“æœåˆ°CSV
# ================================
print(f"\n= ä¿å­˜ç»“æœåˆ°CSV =")

# åˆ›å»ºDataFrame
import pandas as pd
results_df = pd.DataFrame(all_results)

# ä¿å­˜å®Œæ•´ç»“æœ
csv_file = os.path.join(output_dir, "prediction_results.csv")
results_df.to_csv(csv_file, index=False, encoding='utf-8')
print(f"OK å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {csv_file}")

# ç­›é€‰é«˜è´¨é‡åºåˆ—
if prediction_summary['avg_plddt_list']:
    high_quality = results_df[
        (results_df['success'] == True) & 
        (results_df['avg_plddt'] >= plddt_threshold)
    ].copy()
    
    if len(high_quality) > 0:
        high_quality_file = os.path.join(output_dir, f"high_quality_sequences_plddt_{plddt_threshold}.csv")
        high_quality.to_csv(high_quality_file, index=False, encoding='utf-8')
        print(f"OK é«˜è´¨é‡åºåˆ—å·²ä¿å­˜åˆ°: {high_quality_file} ({len(high_quality)})")

# ================================
# 7. å¯è§†åŒ–å’Œå›¾è¡¨ç”Ÿæˆ
# ================================
print(f"\n= ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨ =")

def generate_structure_visualization(result_dir, sequence_id, rank_num=1):
    """ä¸ºå•ä¸ªåºåˆ—ç”Ÿæˆç»“æ„å¯è§†åŒ–"""
    try:
        # æŸ¥æ‰¾PDBæ–‡ä»¶
        pdb_files = glob.glob(os.path.join(result_dir, f"*rank_{rank_num}*unrelaxed*.pdb"))
        if not pdb_files:
            return None
        
        # åˆ›å»º3Då¯è§†åŒ–
        view = py3Dmol.view(js='https://3dmol.org/build/3Dmol.js')
        view.addModel(open(pdb_files[0],'r').read(),'pdb')
        
        # è®¾ç½®æ ·å¼
        view.setStyle({'cartoon': {'colorscheme': {'prop':'b','gradient': 'roygb','min':50,'max':90}}})
        view.zoomTo()
        
        # ä¿å­˜HTML
        html_file = os.path.join(result_dir, f"{sequence_id}_structure_3d.html")
        view.show()
        
        return html_file
    except Exception as e:
        print(f"3Då¯è§†åŒ–å¤±è´¥ {sequence_id}: {e}")
        return None

# ä¸ºæ¯ä¸ªæˆåŠŸé¢„æµ‹çš„åºåˆ—ç”Ÿæˆå¯è§†åŒ–
successful_sequences = [r for r in all_results if r.get('success', False)]

if successful_sequences:
    print(f"ä¸º {len(successful_sequences)} ä¸ªæˆåŠŸé¢„æµ‹çš„åºåˆ—ç”Ÿæˆå¯è§†åŒ–...")
    
    for result in successful_sequences[:5]:  # é™åˆ¶ä¸ºå‰5ä¸ªï¼Œé¿å…è¾“å‡ºè¿‡å¤š
        seq_id = result['sequence_id']
        result_dir = os.path.join(output_dir, seq_id)
        generate_structure_visualization(result_dir, seq_id)

# ================================
# 8. æ‰“åŒ…å’Œä¸‹è½½ç»“æœ
# ================================
print(f"\n= æ‰“åŒ…ä¸‹è½½ç»“æœ =")

results_zip = f"{jobname}_results_{timestamp}.zip"

with zipfile.ZipFile(results_zip, 'w') as zf:
    # æ·»åŠ æ‰€æœ‰æ–‡ä»¶
    for root, dirs, files in os.walk(output_dir):
        for file in files:
            file_path = os.path.join(root, file)
            arcname = os.path.relpath(file_path, '.')
            zf.write(file_path, arcname)
    
    # æ·»åŠ åˆ†ææŠ¥å‘Š
    report_file = f"{jobname}_analysis_report.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(f"# AlphaFold2MSA é¢„æµ‹åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write(f"## é¢„æµ‹å‚æ•°\n")
        f.write(f"- MSAæ¨¡å¼: {msa_mode}\n")
        f.write(f"- æ¨¡æ¿æ¨¡å¼: {template_mode}\n")
        f.write(f"- æ¨¡å‹ç±»å‹: {model_type}\n")
        f.write(f"- å¾ªç¯æ¬¡æ•°: {num_recycles}\n")
        f.write(f"- æœ€å¤§MSA: {max_msa}\n\n")
        f.write(f"## é¢„æµ‹ç»“æœç»Ÿè®¡\n")
        f.write(f"- æ€»åºåˆ—æ•°: {len(all_results)}\n")
        f.write(f"- æˆåŠŸé¢„æµ‹: {prediction_summary['successful']}\n")
        f.write(f"- å¤±è´¥é¢„æµ‹: {prediction_summary['failed']}\n")
        f.write(f"- æˆåŠŸç‡: {prediction_summary['successful']/len(all_results)*100:.1f}%\n\n")
        
        if prediction_summary['avg_plddt_list']:
            f.write(f"## è´¨é‡æŒ‡æ ‡\n")
            f.write(f"- å¹³å‡pLDDT: {np.mean(prediction_summary['avg_plddt_list']):.2f}\n")
            f.write(f"- æœ€é«˜pLDDT: {np.max(prediction_summary['avg_plddt_list']):.2f}\n")
            f.write(f"- æœ€ä½pLDDT: {np.min(prediction_summary['avg_plddt_list']):.2f}\n")
            f.write(f"- è´¨é‡é˜ˆå€¼: {plddt_threshold}\n\n")
        
        f.write(f"## æŒ‰æ–¹æ³•ç»Ÿè®¡\n")
        for method, count in prediction_summary['method_stats'].items():
            f.write(f"- {method}: {count} ä¸ªåºåˆ—\n")
    
    zf.write(report_file, "analysis_report.md")

print(f"OK ç»“æœåŒ…å·²åˆ›å»º: {results_zip}")

# Download file
try:
    from google.colab import files
    files.download(results_zip)
    print(f"OK æ–‡ä»¶å·²ä¸‹è½½: {results_zip}")
except Exception as download_err:
    print(f"è‡ªåŠ¨ä¸‹è½½å¤±è´¥: {download_err}")
    print(f"è¯·æ‰‹åŠ¨ä¸‹è½½æ–‡ä»¶: {results_zip}")
    print("æç¤º: åœ¨Colabå·¦ä¾§æ–‡ä»¶æ ä¸­æ‰¾åˆ°æ–‡ä»¶å¹¶å³é”®é€‰æ‹©ä¸‹è½½")

# ================================
# 9. æœ€ç»ˆæ€»ç»“
# ================================
print("\n" + "="*80)
print("AlphaFold2é¢„æµ‹å®Œæˆ!")
print("="*80)
print(f"é¡¹ç›®: {jobname}")
print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"æˆåŠŸé¢„æµ‹: {prediction_summary['successful']}/{len(all_results)} ä¸ªåºåˆ—")
print(f"è¾“å‡ºç›®å½•: {output_dir}")
print(f"ä¸»è¦æ–‡ä»¶:")
print(f"  - prediction_results.csv: å®Œæ•´é¢„æµ‹ç»“æœ")
print(f"  - high_quality_sequences_plddt_{plddt_threshold}.csv: é«˜è´¨é‡åºåˆ—")
print(f"  - analysis_report.md: è¯¦ç»†åˆ†ææŠ¥å‘Š")
print(f"  - {results_zip}: å®Œæ•´ç»“æœåŒ…")

if prediction_summary['avg_plddt_list']:
    print(f"\nè´¨é‡æ€»ç»“:")
    print(f"  å¹³å‡pLDDT: {np.mean(prediction_summary['avg_plddt_list']):.2f}")
    print(f"  è´¨é‡èŒƒå›´: {np.min(prediction_summary['avg_plddt_list']):.1f} - {np.max(prediction_summary['avg_plddt_list']):.1f}")

print("\né¢„æµ‹å®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹è½½çš„ç»“æœæ–‡ä»¶ã€‚")
print("="*80)